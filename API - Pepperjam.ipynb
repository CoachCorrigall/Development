{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# **Pepperjam API**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "IsFullLoad = False\r\n",
        "Debugging = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Import necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {},
      "source": [
        "from dataclasses import dataclass\n",
        "from datetime import datetime, timedelta, date\n",
        "from pyspark.sql.utils import AnalysisException\n",
        "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, IntegerType, DoubleType\n",
        "from pyspark.sql.functions import col, explode, max, size, when, element_at, struct, lit, udf, from_json\n",
        "from datetime import datetime, date, timedelta\n",
        "from notebookutils import mssparkutils\n",
        "from typing import List\n",
        "from pyspark.sql import SparkSession\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from pprint import pprint\n",
        "\n",
        "import pyspark\n",
        "import requests\n",
        "import json\n",
        "import csv\n",
        "import logging\n",
        "import delta\n",
        "import concurrent.futures\n",
        "\n",
        "# This configuration setting has been added to spark pool configuration, which is applied when the pool is started.\n",
        "# spark.conf.set(\"spark.databricks.delta.replaceWhere.constraintCheck.enabled\", False)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Constants "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "LOAD_BLOCK = timedelta(days=60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Create logger object to record INFO and ERROR events**\r\n",
        "**Logger name must be added to the spark.synapse.logAnalytics.filter.loggerName.match line on the spark pool configuration. Example logger name: \"Awin_Notebook_Logging\" in this code: mylogger = spark_log4j.LogManager.getLogger(\"Awin_Notebook_Logging\")**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {},
      "source": [
        "if Debugging:\n",
        "    mylogger = logging.getLogger('azure.synapse.pyspark')\n",
        "    mylogger.setLevel(logging.DEBUG) # Change this log level to reduce or increase the amount of logging: DEBUG, INFO, WARNING, ERROR.\n",
        "    mylogger.handlers.clear()\n",
        "    mylogger.addHandler(logging.StreamHandler())\n",
        "    mylogger.propagate = False\n",
        "else:\n",
        "    spark_log4j = sc._jvm.org.apache.log4j\n",
        "    mylogger = spark_log4j.LogManager.getLogger(\"Pepperjam_Notebook_Logging\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **Run the KeyVaultSecrets notebook to gain access to the list of key vault secrets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {},
      "source": [
        "%run KeyVaultSecrets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Classes to handle the client objects and error responses**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {},
      "source": [
        "@dataclass\n",
        "class Client: #This will be specific to the api. The initial fields were copied from CJ, but now I want to grab the necessary fields for Pepperjam (PJ) \n",
        "    ProgramName: str\n",
        "    apiKey: str\n",
        "    ProgramId: str\n",
        "    LoadParams: LoadParams = None  \n",
        "\n",
        "@dataclass\n",
        "class LoadParams:\n",
        "    IsDelta: bool\n",
        "    Endpoint: str\n",
        "    DLPath_STG: str\n",
        "    DLPath_PSA: str\n",
        "    DateStart: datetime\n",
        "    DateEnd: datetime\n",
        "    MergeKeys: str = None\n",
        "    DeltaColumn: str = None\n",
        "    RelativeDate: int = None\n",
        "    QueryParams: str = ''\n",
        "\n",
        "class APIError(Exception):\n",
        "    \"\"\"Base class for API-related exceptions.\"\"\"\n",
        "    pass\n",
        "\n",
        "class APIResponseError(APIError):\n",
        "    def __init__(self, status_code,ProgramId,endpoint, message,client=None, details=None):\n",
        "        self.status_code = status_code\n",
        "        self.ProgramId = ProgramId\n",
        "        self.endpoint = endpoint\n",
        "        self.message = message\n",
        "        self.details = details\n",
        "        self.client = client\n",
        "\n",
        "    def __str__(self):\n",
        "        base_msg = f\"APIResponseError for program ({self.ProgramId}) on endpoint ({self.endpoint}) raised with status {self.status_code}: {self.message}\"\n",
        "        if self.details:\n",
        "            base_msg += f\"\\nDetails: {self.details}\\nEndpoint: {self.endpoint}\"\n",
        "        return base_msg\n",
        "\n",
        "class APITokenError(APIError):\n",
        "    def __init__(self, message, client=None,details=None):\n",
        "        self.message = message\n",
        "        self.details = details\n",
        "        self.client = client\n",
        "\n",
        "    def __str__(self):\n",
        "        client_msg = f\" for client ({self.client.ProgramName})-{self.client.ProgramId}\" if self.client else \"\"\n",
        "        base_msg = f\"APITokenError{client_msg} raised: {self.message}\"\n",
        "        if self.details:\n",
        "            base_msg += f\"\\nDetails: {self.details}\"\n",
        "        return base_msg\n",
        "\n",
        "class APIProgramIdError(APIError):\n",
        "    def __init__(self, ProgramId,client=None,details=None):\n",
        "        self.ProgramId = ProgramId\n",
        "        self.details = details\n",
        "        self.client = client\n",
        "\n",
        "    def __str__(self):\n",
        "        client_msg = f\" for client ({self.client.ProgramName})-{self.client.ProgramId}\" if self.client else \"\"\n",
        "        base_msg = f'APIProgramIdError{client_msg} raised: ProgramId \"{self.ProgramId}\" is invalid'\n",
        "        if self.details:\n",
        "            base_msg += f\"\\nDetails: {self.details}\"\n",
        "        return base_msg\n",
        "\n",
        "class APIDeserializeError(APIError):\n",
        "    def __init__(self, client=None,details=None):\n",
        "        self.details = details\n",
        "\n",
        "    def __str__(self):\n",
        "        client_msg = f\" for client ({self.client.ProgramName})-{self.client.ProgramId}\" if self.client else \"\"\n",
        "        base_msg = f'APIDeserializeError{client_msg} raised'\n",
        "        if self.details:\n",
        "            base_msg += f\"\\nDetails: {self.details}\"\n",
        "        return base_msg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Pepperjam (PJ) Data Access Class**   \n",
        "_Includes all methods to retrieve and deserialize commission junction API's_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {},
      "source": [
        "class PJ_DataAccess:\n",
        "    # Sends a single API request to advertiser endpoint arg, when addtl query params are not needed\n",
        "    def getData(self, client: Client, end_point: str) -> list[dict]:   \n",
        "        mylogger.debug(f'Retrieving {end_point}: for {client.ProgramId}')\n",
        "        records = []\n",
        "        moreRecords = True\n",
        "        \n",
        "        endpoint = end_point\n",
        "        base_url = f'https://api.pepperjamnetwork.com/20120402/advertiser/{endpoint}?apiKey={client.apiKey}{client.LoadParams.QueryParams}&format=json'\n",
        "\n",
        "        mylogger.info(f'...Calling {endpoint} for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "        while moreRecords == True:\n",
        "            try:\n",
        "                resp = requests.get(url = base_url)\n",
        "        \n",
        "                # Convert response.text to json object\n",
        "                json_response = json.loads(resp.text)\n",
        "\n",
        "                if resp.status_code == 429:\n",
        "                    raise APIResponseError(status_code = resp.status_code,ProgramId = client.ProgramId, endpoint= endpoint, message=resp.reason)\n",
        "\n",
        "                elif resp.status_code != 200:\n",
        "                    raise APIResponseError(status_code = resp.status_code,ProgramId = client.ProgramId, endpoint= endpoint, message=resp.reason)\n",
        "                \n",
        "            except APIError as e:\n",
        "                mylogger.error(str(e))\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                # Add records to records list\n",
        "                records.extend(json_response['data'])\n",
        "        \n",
        "            # Assign next href for pagination\n",
        "            if \"next\" in json_response[\"meta\"]['pagination']:\n",
        "                base_url = json_response[\"meta\"]['pagination']['next']['href']\n",
        "            else:\n",
        "                moreRecords = False\n",
        "        return records\n",
        "\n",
        "    # Sends API request to advertiser end_point argument, when start and end date are required\n",
        "    def getParamData(self, client: Client, start_date: str, end_date: str, end_point: str) -> list[dict]: \n",
        "        records = []\n",
        "        moreRecords = True\n",
        "    \n",
        "        endpoint = end_point       #Add actual endpoint from postman\n",
        "        startDate = start_date\n",
        "        endDate = end_date\n",
        "        base_url = f'https://api.pepperjamnetwork.com/20120402/advertiser/{endpoint}?apiKey={client.apiKey}{client.LoadParams.QueryParams}&format=json&startDate={startDate}&endDate={endDate}'\n",
        "        \n",
        "        mylogger.info(f'...Calling {endpoint} for {client.ProgramName}-{client.ProgramId}. Start Date {startDate} End Date {endDate}')\n",
        "\n",
        "        while moreRecords == True:\n",
        "            try:\n",
        "                resp = requests.get(url = base_url)\n",
        "        \n",
        "                # Convert response.text to json object\n",
        "                json_response = json.loads(resp.text)\n",
        "\n",
        "                if resp.status_code == 429:\n",
        "                    raise APIResponseError(status_code = resp.status_code,ProgramId = client.ProgramId, endpoint= endpoint, message=resp.reason)\n",
        "\n",
        "                elif resp.status_code != 200:\n",
        "                    raise APIResponseError(status_code = resp.status_code,ProgramId = client.ProgramId, endpoint= endpoint, message=resp.reason)\n",
        "                \n",
        "            except APIError as e:\n",
        "                mylogger.error(str(e))\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                # Add records to records list\n",
        "                records.extend(json_response['data'])\n",
        "        \n",
        "            # Assign next href for pagination\n",
        "            if \"next\" in json_response[\"meta\"]['pagination']:\n",
        "                base_url = json_response[\"meta\"]['pagination']['next']['href']\n",
        "            else:\n",
        "                moreRecords = False\n",
        "    \n",
        "        return records \n",
        "\n",
        "    # Send an API request with no pagination (advertiser creative)\n",
        "    def getNoPagination(self, client: Client, end_point:str) -> list[dict]: \n",
        "        records = []\n",
        "        moreRecords = True\n",
        "        \n",
        "        endpoint = end_point\n",
        "        base_url = f'https://api.pepperjamnetwork.com/20120402/advertiser/{endpoint}?apiKey={client.apiKey}&format=json'\n",
        "\n",
        "        resp = requests.get(url = base_url)\n",
        "        json_response = json.loads(resp.text)\n",
        "        records.extend(json_response['data'])\n",
        "        return records"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### **Pepperjam (PJ) Deserialize Class**   \r\n",
        "_Methods to convert to RDD and deserialize PepperJam API data_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "class PJ_Deserialize:\r\n",
        "    def _ConvertToPysparkRDD(self, json_list: list[dict]) -> pyspark.rdd.PipelinedRDD:\r\n",
        "        # Converts the list of dictionaries returned by the get call into a Resilient Distributed Dataset (rdd)\r\n",
        "        rdd = spark.sparkContext.parallelize(json_list)\r\n",
        "        json_rdd = rdd.map(lambda x: json.dumps(x))\r\n",
        "        \r\n",
        "        return json_rdd\r\n",
        "\r\n",
        "\r\n",
        "    # deserialize data\r\n",
        "    def ds(self, json_list: list[dict]) -> pyspark.sql.dataframe.DataFrame:\r\n",
        "        json_rdd = self._ConvertToPysparkRDD(json_list)\r\n",
        "        json_df = spark.read.json(json_rdd)\r\n",
        "        \r\n",
        "        if \"promotions\" in json_df.columns:\r\n",
        "            json_df = json_df.drop(json_df.promotions)\r\n",
        "        if \"private_affiliates\" in json_df.columns:\r\n",
        "            json_df = json_df.drop(json_df.private_affiliates)\r\n",
        "        return json_df\r\n",
        "\r\n",
        "    def banner_ds(self, json_list: list[dict]) -> pyspark.sql.dataframe.DataFrame:\r\n",
        "        json_rdd = self._ConvertToPysparkRDD(json_list)\r\n",
        "        json_df = spark.read.json(json_rdd)\r\n",
        "        \r\n",
        "        if \"promotions\" in json_df.columns:\r\n",
        "            json_df = json_df.drop(json_df.promotions)\r\n",
        "        if \"private_affiliates\" in json_df.columns:\r\n",
        "            json_df = json_df.drop(json_df.private_affiliates)\r\n",
        "        return json_df\r\n",
        "    \r\n",
        "    def banner_promotion_ds(self, json_list: list[dict]) -> pyspark.sql.dataframe.DataFrame:\r\n",
        "        json_rdd = self._ConvertToPysparkRDD(json_list)\r\n",
        "        json_rdd_filtered = json_rdd.filter(lambda x: len(json.loads(x).get('promotions') or []) > 0)\r\n",
        "        json_df = spark.read.json(json_rdd_filtered)\r\n",
        "\r\n",
        "\r\n",
        "        if \"promotions\" in json_df.columns:\r\n",
        "            json_df = json_df.select(\r\n",
        "                \"id\",\r\n",
        "                explode(col(\"promotions\")).alias(\"promotions\")\r\n",
        "            )\r\n",
        "            json_df = json_df.select(\r\n",
        "                col(\"id\").alias(\"creative_banner_id\"),\r\n",
        "                col(\"promotions.id\").alias(\"promotion_id\"),\r\n",
        "                col(\"promotions.name\").alias(\"promotion_name\"),\r\n",
        "            )\r\n",
        "            return json_df\r\n",
        "        else:\r\n",
        "            return None\r\n",
        "\r\n",
        "    def banner_private_aff_ds(self,json_list: list[dict]) -> pyspark.sql.dataframe.DataFrame:\r\n",
        "        json_rdd = self._ConvertToPysparkRDD(json_list)\r\n",
        "        json_df = spark.read.json(json_rdd)\r\n",
        "\r\n",
        "        if \"private_affiliates\" in json_df.columns:\r\n",
        "            json_df = json_df.select(\r\n",
        "                \"id\",\r\n",
        "                explode(col(\"private_affiliates\")).alias(\"private_affiliates\")\r\n",
        "            )\r\n",
        "            json_df = json_df.select(\r\n",
        "                col(\"id\").alias(\"creative_banner_id\"),\r\n",
        "                col(\"private_affiliates.affiliate_id\").alias(\"private_affiliate_id\"),\r\n",
        "                col(\"private_affiliates.company_name\").alias(\"private_affiliate_company_name\"),\r\n",
        "                col(\"private_affiliates.first_name\").alias(\"private_affiliate_first_name\"),\r\n",
        "                col(\"private_affiliates.last_name\").alias(\"private_affiliate_last_name\"),\r\n",
        "            )\r\n",
        "            return json_df\r\n",
        "        else:\r\n",
        "            return None\r\n",
        "\r\n",
        "    def coupon_ds(self, json_list: list[dict]) -> pyspark.sql.dataframe.DataFrame:\r\n",
        "        json_rdd = self._ConvertToPysparkRDD(json_list)\r\n",
        "        json_df = spark.read.json(json_rdd)\r\n",
        "        if \"promotions\" in json_df.columns:\r\n",
        "            json_df = json_df.drop(json_df.promotions)\r\n",
        "        if \"private_affiliates\" in json_df.columns:\r\n",
        "            json_df = json_df.drop(json_df.private_affiliates)\r\n",
        "        return json_df\r\n",
        "\r\n",
        "\r\n",
        "    def ad_publisher_ds(self, json_list: list[dict]) -> pyspark.sql.dataframe.DataFrame:\r\n",
        "        json_rdd = self._ConvertToPysparkRDD(json_list)\r\n",
        "        json_df = spark.read.json(json_rdd)\r\n",
        "        if \"category\" in json_df.columns:\r\n",
        "            json_df = json_df.drop(json_df.category)\r\n",
        "        if \"promotional_method\" in json_df.columns:\r\n",
        "            json_df = json_df.drop(json_df.promotional_method)\r\n",
        "        return json_df\r\n",
        "\r\n",
        "    def coupon_promotion_ds(self, json_list: list[dict]) -> pyspark.sql.dataframe.DataFrame:\r\n",
        "        json_rdd = self._ConvertToPysparkRDD(json_list)\r\n",
        "        json_rdd_filtered = json_rdd.filter(lambda x: len(json.loads(x).get('promotions') or []) > 0)\r\n",
        "        json_df = spark.read.json(json_rdd_filtered)\r\n",
        "\r\n",
        "\r\n",
        "        if \"promotions\" in json_df.columns:\r\n",
        "            json_df = json_df.select(\r\n",
        "                \"id\",\r\n",
        "                explode(col(\"promotions\")).alias(\"promotions\")\r\n",
        "            )\r\n",
        "            json_df = json_df.select(\r\n",
        "                col(\"id\").alias(\"creative_coupon_id\"),\r\n",
        "                col(\"promotions.id\").alias(\"promotion_id\"),\r\n",
        "                col(\"promotions.name\").alias(\"promotion_name\"),\r\n",
        "            )\r\n",
        "            return json_df\r\n",
        "        else:\r\n",
        "            return None\r\n",
        "\r\n",
        "    def coupon_private_aff_ds(self,json_list: list[dict]) -> pyspark.sql.dataframe.DataFrame:\r\n",
        "        json_rdd = self._ConvertToPysparkRDD(json_list)\r\n",
        "        json_df = spark.read.json(json_rdd)\r\n",
        "        if \"private_affiliates\" in json_df.columns:\r\n",
        "            json_df = json_df.select(\r\n",
        "                \"id\",\r\n",
        "                explode(col(\"private_affiliates\")).alias(\"private_affiliates\")\r\n",
        "            )\r\n",
        "            json_df = json_df.select(\r\n",
        "                col(\"id\").alias(\"creative_coupon_id\"),\r\n",
        "                col(\"private_affiliates.affiliate_id\").alias(\"private_affiliate_id\"),\r\n",
        "                col(\"private_affiliates.company_name\").alias(\"private_affiliate_company_name\"),\r\n",
        "                col(\"private_affiliates.first_name\").alias(\"private_affiliate_first_name\"),\r\n",
        "                col(\"private_affiliates.last_name\").alias(\"private_affiliate_last_name\"),\r\n",
        "            )\r\n",
        "            return json_df\r\n",
        "        else:\r\n",
        "            return None\r\n",
        "\r\n",
        "    def ad_publisher_category_ds(self,json_list: list[dict]) -> pyspark.sql.dataframe.DataFrame:\r\n",
        "        json_rdd = self._ConvertToPysparkRDD(json_list)\r\n",
        "        json_df = spark.read.json(json_rdd)\r\n",
        "        json_df = json_df.withColumnRenamed(\"id\", \"advertiser_publisher_id\")\r\n",
        "\r\n",
        "        if(\"category\") in json_df.columns:\r\n",
        "            json_df = json_df.select(\r\n",
        "                \"advertiser_publisher_id\",\r\n",
        "                explode(col(\"category\")).alias(\"category\")\r\n",
        "            )\r\n",
        "            json_df = json_df.select(\r\n",
        "                col(\"advertiser_publisher_id\").alias(\"advertiser_publisher_id\"),\r\n",
        "                col(\"category.id\").alias(\"category_id\"),\r\n",
        "                col(\"category.name\").alias(\"category_name\"),\r\n",
        "            )\r\n",
        "            return json_df\r\n",
        "        else:\r\n",
        "            return None\r\n",
        "\r\n",
        "    def ad_publisher_promo_method_ds(self,json_list: list[dict]) -> pyspark.sql.dataframe.DataFrame:\r\n",
        "        json_rdd = self._ConvertToPysparkRDD(json_list)\r\n",
        "        json_df = spark.read.json(json_rdd)\r\n",
        "        json_df = json_df.withColumnRenamed(\"id\", \"advertiser_publisher_id\")\r\n",
        "\r\n",
        "        if(\"promotional_method\") in json_df.columns:\r\n",
        "            json_df = json_df.select(\r\n",
        "                \"advertiser_publisher_id\",\r\n",
        "                explode(col(\"promotional_method\")).alias(\"promotional_method\")\r\n",
        "            )\r\n",
        "            json_df = json_df.select(\r\n",
        "                col(\"advertiser_publisher_id\").alias(\"advertiser_publisher_id\"),\r\n",
        "                col(\"promotional_method.id\").alias(\"promotional_method_id\"),\r\n",
        "                col(\"promotional_method.name\").alias(\"promotional_method_name\"),\r\n",
        "            )\r\n",
        "            return json_df\r\n",
        "        else:\r\n",
        "            return None   \r\n",
        "\r\n",
        "    def creative_text_affiliates_ds(self,json_list: list[dict]) -> pyspark.sql.dataframe.DataFrame:\r\n",
        "        json_rdd = self._ConvertToPysparkRDD(json_list)\r\n",
        "        json_df = spark.read.json(json_rdd)\r\n",
        "        json_df = json_df.withColumnRenamed(\"id\", \"advertiser_id\")\r\n",
        "\r\n",
        "        if \"private_affiliates\" in json_df.columns:\r\n",
        "            json_df = json_df.select(\r\n",
        "                \"advertiser_id\",\r\n",
        "                explode(col(\"private_affiliates\")).alias(\"private_affiliates\")\r\n",
        "            )\r\n",
        "            json_df = json_df.select(\r\n",
        "                col(\"advertiser_id\").alias(\"advertiser_id\"),\r\n",
        "                col(\"private_affiliates.affiliate_id\").alias(\"private_affiliate_id\"),\r\n",
        "                col(\"private_affiliates.company_name\").alias(\"private_affiliate_company_name\"),\r\n",
        "                col(\"private_affiliates.first_name\").alias(\"private_affiliate_first_name\"),\r\n",
        "                col(\"private_affiliates.last_name\").alias(\"private_affiliate_last_name\"),\r\n",
        "            )\r\n",
        "            return json_df\r\n",
        "        else:\r\n",
        "            return None\r\n",
        "\r\n",
        "    def creative_text_promo_ds(self,json_list: list[dict]) -> pyspark.sql.dataframe.DataFrame:\r\n",
        "        json_rdd = self._ConvertToPysparkRDD(json_list)\r\n",
        "        json_rdd_filtered = json_rdd.filter(lambda x: len(json.loads(x).get('promotions') or []) > 0)\r\n",
        "        json_df = spark.read.json(json_rdd_filtered)\r\n",
        "        json_df = json_df.withColumnRenamed(\"id\", \"advertiser_id\")\r\n",
        "\r\n",
        "        if(\"promotions\") in json_df.columns:\r\n",
        "            json_df = json_df.select(\r\n",
        "                \"advertiser_id\",\r\n",
        "                explode(col(\"promotions\")).alias(\"promotions\")\r\n",
        "            )\r\n",
        "            json_df = json_df.select(\r\n",
        "                col(\"advertiser_id\").alias(\"advertiser_id\"),\r\n",
        "                col(\"promotions.id\").alias(\"promotions_id\"),\r\n",
        "                col(\"promotions.name\").alias(\"promotions_name\"),\r\n",
        "            )\r\n",
        "            return json_df\r\n",
        "        else:\r\n",
        "            return None\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {},
      "source": [
        "class PJ_ControlLogic:\n",
        "    def __init__(self):\n",
        "        self.clientSecretValues = self._GetClientSecretValues()\n",
        "        self.dataAccess = PJ_DataAccess()\n",
        "        self.deserialize = PJ_Deserialize()\n",
        "        self.container = 'datalake'\n",
        "        self.datalakename = 'gen3dlsprod'\n",
        "\n",
        "    #------------------------------- PRIVATE METHODS -------------------------------#\n",
        "    def _GetClientSecretValues(self):\n",
        "        kvName = 'gen3-PepperJam-kv'\n",
        "        kv = KeyVaultValues()\n",
        "        clientSecretValues = []\n",
        "        clientSecretNames = kv.GetKVSecretList(kvName)\n",
        "\n",
        "        for secret in clientSecretNames:\n",
        "            secretValue = kv.GetKVSecretJSON(kvName, secret['SecretName'], 'KeyVault_Master' )\n",
        "            if secretValue.get('Program Name') is not None:\n",
        "                clientSecretValues.append(\n",
        "                    Client(\n",
        "                        ProgramName = secretValue.get('Program Name').replace(' ','').replace('/','') if secretValue.get('Program Name') is not None else 'NotDefined',\n",
        "                        apiKey = secretValue.get('API Key'),\n",
        "                        ProgramId =secretValue.get('Program ID')\n",
        "                    )\n",
        "                )\n",
        "        return clientSecretValues\n",
        "\n",
        "    def _SetDateStartEnd(self, client: Client) -> None:\n",
        "        fullLoadStartDate = date(2020, 8, 1)\n",
        "\n",
        "        if IsFullLoad or client.LoadParams.IsDelta == False or mssparkutils.fs.exists(client.LoadParams.DLPath_PSA) == False:\n",
        "            mylogger.info(f'...delta loading unavailable or toggled off for {client.ProgramName}-{client.ProgramId} {client.LoadParams.Endpoint}')\n",
        "            client.LoadParams.DateStart = fullLoadStartDate\n",
        "            mylogger.info(f'...loading data since {client.LoadParams.DateStart}')\n",
        "\n",
        "        else:\n",
        "            mylogger.info(f'...retrieving max_delta_value for {client.ProgramName}-{client.ProgramId} {client.LoadParams.Endpoint} from PSA')\n",
        "            # Load the delta file into a dataframe\n",
        "            df_destination_delta = spark.read.format(\"delta\").load(client.LoadParams.DLPath_PSA)\n",
        "\n",
        "            # Find the maximum delta column value\n",
        "            max_delta_value = df_destination_delta.select(max(f\"{client.LoadParams.DeltaColumn}\")).first()[0]\n",
        "\n",
        "            # Parse data type to [date], truncating any timestamp\n",
        "            client.LoadParams.DateStart = datetime.strptime(max_delta_value.split(' ')[0], '%Y-%m-%d').date()\n",
        "            client.LoadParams.DateStart = client.LoadParams.DateStart - LOAD_BLOCK\n",
        "            mylogger.info(f'...loading data since {client.LoadParams.DateStart}')\n",
        "\n",
        "        client.LoadParams.DateEnd = datetime.utcnow().date()\n",
        "\n",
        "# TIME CLASS\n",
        "    def _GetRelativeStartEndDates(self, client: Client) -> list[tuple]:\n",
        "        # CHUNK ONE\n",
        "        startDate       = client.LoadParams.DateStart\n",
        "        startDateStr    = (datetime.strftime(startDate, '%Y-%m-%d'))\n",
        "        dateDelta = relativedelta(months=client.LoadParams.RelativeDate) # CHUNK SIZE\n",
        "        endDate = startDate + dateDelta\n",
        "        endDateStr = (datetime.strftime(endDate, '%Y-%m-%d'))\n",
        "\n",
        "        currDay = date.today() # call today once to avoid constant time updates\n",
        "        DateBounds = []\n",
        "        \n",
        "        if (endDate > currDay):\n",
        "            endDate = currDay\n",
        "            endDateStr = (datetime.strftime(endDate, '%Y-%m-%d'))\n",
        "        \n",
        "        DateBounds.append((startDateStr, endDateStr))\n",
        "        while (endDate < currDay):\n",
        "\n",
        "            startDate = endDate + relativedelta(days=1)\n",
        "            startDateStr = (datetime.strftime(startDate, '%Y-%m-%d'))\n",
        "            \n",
        "            endDate = startDate + dateDelta\n",
        "            endDateStr = (datetime.strftime(endDate, '%Y-%m-%d'))\n",
        "\n",
        "            if (endDate > currDay):\n",
        "                endDate = currDay\n",
        "                endDateStr = (datetime.strftime(endDate, '%Y-%m-%d'))\n",
        "\n",
        "            DateBounds.append((startDateStr, endDateStr))\n",
        "\n",
        "        \n",
        "        return DateBounds\n",
        "\n",
        "    def _Get1MStartEndDates(self, client: Client) -> list[tuple]:\n",
        "        # CHUNK ONE\n",
        "        startDate       = (datetime.strptime('2020-08-01', '%Y-%m-%d'))\n",
        "        startDateStr    = (datetime.strftime(startDate, '%Y-%m-%d'))\n",
        "        dateDelta = relativedelta(months=1) # CHUNK SIZE\n",
        "        endDate = startDate + dateDelta\n",
        "        endDateStr = (datetime.strftime(endDate, '%Y-%m-%d'))\n",
        "\n",
        "        currDay = datetime.today() # call today once to avoid constant time updates\n",
        "        DateBounds = []\n",
        "\n",
        "        DateBounds.append((startDateStr, endDateStr))\n",
        "        while (endDate < currDay):\n",
        "\n",
        "            startDate = endDate + relativedelta(days=1)\n",
        "            startDateStr = (datetime.strftime(startDate, '%Y-%m-%d'))\n",
        "            \n",
        "            endDate = startDate + dateDelta\n",
        "            endDateStr = (datetime.strftime(endDate, '%Y-%m-%d'))\n",
        "\n",
        "            if (endDate > currDay):\n",
        "                endDate = currDay\n",
        "                endDateStr = (datetime.strftime(endDate, '%Y-%m-%d'))\n",
        "\n",
        "            DateBounds.append((startDateStr, endDateStr))\n",
        "\n",
        "        \n",
        "        return DateBounds\n",
        "\n",
        "    def _Get6MStartEndDates(self, client: Client) -> list[tuple]:\n",
        "        # CHUNK ONE\n",
        "        startDate       = (datetime.strptime('2020-08-01', '%Y-%m-%d'))\n",
        "        startDateStr    = (datetime.strftime(startDate, '%Y-%m-%d'))\n",
        "        dateDelta = relativedelta(months=6) # CHUNK SIZE\n",
        "        endDate = startDate + dateDelta\n",
        "        endDateStr = (datetime.strftime(endDate, '%Y-%m-%d'))\n",
        "\n",
        "        currDay = datetime.today() # call today once to avoid constant time updates\n",
        "        DateBounds = []\n",
        "\n",
        "        DateBounds.append((startDateStr, endDateStr))\n",
        "        while (endDate < currDay):\n",
        "\n",
        "            startDate = endDate + relativedelta(days=1)\n",
        "            startDateStr = (datetime.strftime(startDate, '%Y-%m-%d'))\n",
        "            \n",
        "            endDate = startDate + dateDelta\n",
        "            endDateStr = (datetime.strftime(endDate, '%Y-%m-%d'))\n",
        "\n",
        "            if (endDate > currDay):\n",
        "                endDate = currDay\n",
        "                endDateStr = (datetime.strftime(endDate, '%Y-%m-%d'))\n",
        "\n",
        "            DateBounds.append((startDateStr, endDateStr))\n",
        "\n",
        "        \n",
        "        return DateBounds\n",
        "\n",
        "    def _Get1YStartEndDates(self, client: Client) -> list[tuple]:\n",
        "            # CHUNK ONE\n",
        "            startDate       = (datetime.strptime('2020-08-01', '%Y-%m-%d'))\n",
        "            startDateStr    = (datetime.strftime(startDate, '%Y-%m-%d'))\n",
        "            dateDelta = relativedelta(years=1) # CHUNK SIZE\n",
        "            endDate = startDate + dateDelta\n",
        "            endDateStr = (datetime.strftime(endDate, '%Y-%m-%d'))\n",
        "\n",
        "            currDay = datetime.today() # call today once to avoid constant time updates\n",
        "            DateBounds = []\n",
        "\n",
        "            DateBounds.append((startDateStr, endDateStr))\n",
        "            while (endDate < currDay):\n",
        "\n",
        "                startDate = endDate + relativedelta(days=1)\n",
        "                startDateStr = (datetime.strftime(startDate, '%Y-%m-%d'))\n",
        "                \n",
        "                endDate = startDate + dateDelta\n",
        "                endDateStr = (datetime.strftime(endDate, '%Y-%m-%d'))\n",
        "\n",
        "                if (endDate > currDay):\n",
        "                    endDate = currDay\n",
        "                    endDateStr = (datetime.strftime(endDate, '%Y-%m-%d'))\n",
        "\n",
        "                DateBounds.append((startDateStr, endDateStr))\n",
        "\n",
        "            \n",
        "            return DateBounds\n",
        "    \n",
        "    def _getLatestMonth(self, client: Client) -> list[tuple]:\n",
        "        startDate = date.today() + relativedelta(months= -1)\n",
        "        endDate = date.today()\n",
        "        return startDate, endDate\n",
        "\n",
        "    def _WriteDataFrameToLake(self, df, client: Client) -> None:\n",
        "        mylogger.info(f'...writing {client.LoadParams.Endpoint} data to DataLake for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "        if client.LoadParams.IsDelta == True:\n",
        "\n",
        "            # Add ETLImportDate & Write file to STG folder in lake\n",
        "            df = df.withColumn('ETLImportDate', lit(datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')))\n",
        "            df.write.mode('overwrite').parquet(client.LoadParams.DLPath_STG)\n",
        "\n",
        "            # Run the merge activity to move table to PSA\n",
        "            self._MergeDataToPSA(client)\n",
        "        else:\n",
        "\n",
        "            # Add ETLImportDate & Write file to PSA folder in lake\n",
        "            df = df.withColumn('ETLImportDate', lit(datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')))\n",
        "            if mssparkutils.fs.exists(client.LoadParams.DLPath_PSA):\n",
        "                mssparkutils.fs.rm(client.LoadParams.DLPath_PSA, True)\n",
        "            df.write.format('delta').mode('overwrite').save(client.LoadParams.DLPath_PSA)\n",
        "\n",
        "        mylogger.info(f'File overwrite complete: {client.LoadParams.Endpoint} for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "    def _MergeDataToPSA(self, client) -> None:\n",
        "        # Load the stg Parquet file to a dataframe\n",
        "        source_parquet = spark.read.load(client.LoadParams.DLPath_STG, format='parquet')\n",
        "\n",
        "        if mssparkutils.fs.exists(client.LoadParams.DLPath_PSA) and client.LoadParams.IsDelta:\n",
        "            mylogger.info(f'...Merging data to PSA: {client.LoadParams.Endpoint} for {client.ProgramName}-{client.ProgramId}')\n",
        "            # Load the psa Delta file\n",
        "            destination_delta = delta.DeltaTable.forPath(spark, client.LoadParams.DLPath_PSA)\n",
        "\n",
        "            if client.LoadParams.MergeKeys != \"\":\n",
        "                # Merge STG into PSA\n",
        "                (\n",
        "                    destination_delta.alias('t')\n",
        "                        .merge(\n",
        "                            source_parquet.alias('s'),\n",
        "                            client.LoadParams.MergeKeys\n",
        "                            )\n",
        "                        .whenMatchedUpdateAll()\n",
        "                    .whenNotMatchedInsertAll()\n",
        "                    .execute()\n",
        "                )\n",
        "            else:\n",
        "                # No Merge keys available, but still delta load while replacing any records from our StartDate (to avoid duplicates in overlapping load periods).\n",
        "                # !! Important - the spark configuration line in the top cell to disable the \"replace where check constraint\" is what allows non matching records\n",
        "                #                in the replaceWhere constraint to be inserted. It must disabled for this to work properly. !!\n",
        "                dateStart = client.LoadParams.DateStart   #.strftime(f'{client.LoadParams.DeltaValueFormat}')\n",
        "                dateEnd   = client.LoadParams.DateEnd     #.strftime(f'{client.LoadParams.DeltaValueFormat}')\n",
        "                deltaCol = 'cast(' + client.LoadParams.DeltaColumn + ' as date)'\n",
        "                #mylogger.info(dateStart)\n",
        "                #mylogger.info(dateEnd)\n",
        "                #mylogger.info(deltaCol)\n",
        "                (\n",
        "                    source_parquet\n",
        "                        .write\n",
        "                        .format(\"delta\")\n",
        "                        .mode(\"overwrite\")\n",
        "                        .option(\"replaceWhere\", f\"{deltaCol} >= '{dateStart}' AND {deltaCol} <= '{dateEnd}'\") #update \n",
        "                        .save(client.LoadParams.DLPath_PSA)\n",
        "                )\n",
        "        else:\n",
        "            mylogger.info(f'...Initial Load\\n ...Creating PSA file in \"DELTA\" format: {client.LoadParams.Endpoint} for {client.ProgramName}-{client.ProgramId}')\n",
        "            # Create dest delta table (psa)\n",
        "            (\n",
        "                source_parquet\n",
        "                    .write\n",
        "                    .format(\"delta\")\n",
        "                    .mode(\"overwrite\") # if exists, overwrite the delta file\n",
        "                    .option(\"overwriteSchema\", \"True\")\n",
        "                    .save(client.LoadParams.DLPath_PSA) #destination\n",
        "            )\n",
        "\n",
        "        mylogger.info(f'Merge activity complete: {client.LoadParams.Endpoint} for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "    #------------------------------- PUBLIC METHODS -------------------------------#\n",
        "    \n",
        "    ## COPY ACTIVITIES ##\n",
        "    # we added a '_' to file endpoints with '/' so folders aren't created in the lake. endpoints with no '/' are the same as the file\n",
        "\n",
        "    \n",
        "    #------ AdvertiserPublisher -----#\n",
        "    def CopyDataToLake_AdvertiserPublisher(self, client: Client):\n",
        "        endpoint = 'publisher'\n",
        "        #Set Load Params\n",
        "        client.LoadParams = LoadParams(\n",
        "            IsDelta = False,\n",
        "            Endpoint = {endpoint},\n",
        "            DLPath_STG = f'/Pepperjam/{client.ProgramId}/STG/{endpoint}',\n",
        "            DLPath_PSA = f'/Pepperjam/{client.ProgramId}/PSA/{endpoint}',\n",
        "            DateStart = None,\n",
        "            DateEnd = None,\n",
        "            DeltaColumn = '',\n",
        "            MergeKeys = 't.id = s.id',\n",
        "            RelativeDate = None,\n",
        "            QueryParams = ''\n",
        "        )\n",
        "\n",
        "        #Build category Id's into Load Params\n",
        "        categories = self.dataAccess.getData(client = client, end_point='publisher/category')\n",
        "\n",
        "        categoryIds = [record[\"id\"] for record in categories]\n",
        "        queryParam = \"&categoryId=\" + \",\".join(categoryIds)\n",
        "\n",
        "        client.LoadParams.QueryParams = queryParam\n",
        "\n",
        "        #Build promotional_methods into Load Params\n",
        "        promo_methods = self.dataAccess.getData(client = client, end_point='publisher/promotional-method')\n",
        "\n",
        "        promoMethodIds = [record[\"id\"] for record in promo_methods]\n",
        "        queryParam = \"&promotionalMethodId=\" + \",\".join(promoMethodIds)\n",
        "    \n",
        "        all_records = self.dataAccess.getData(client = client, end_point = endpoint)\n",
        "\n",
        "        if len(all_records) > 0:\n",
        "            mylogger.info(f'...Deserializing advertiser/publisher data...')\n",
        "            df = self.deserialize.ad_publisher_ds(all_records)\n",
        "            # Ensure the dataframe is not NULL and that all_records is the correct data type\n",
        "            if type(df) != type(None) and not isinstance(all_records, str):\n",
        "                self._WriteDataFrameToLake(df, client)\n",
        "\n",
        "                self.CopyDataToLake_AdvertiserPublisherCategory(client,all_records)\n",
        "\n",
        "                self.CopyDataToLake_AdvertiserPublisherPromotionalMethod(client,all_records)\n",
        "            else:\n",
        "                mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "        else:\n",
        "            mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')    \n",
        "\n",
        "    def CopyDataToLake_AdvertiserPublisherCategory(self, client: Client,datalist):\n",
        "        endpoint = 'publisher_category'\n",
        "        #Set Load Params\n",
        "        client.LoadParams = LoadParams(\n",
        "            IsDelta = False,\n",
        "            Endpoint = {endpoint},\n",
        "            DLPath_STG = f'/Pepperjam/{client.ProgramId}/STG/{endpoint}',\n",
        "            DLPath_PSA = f'/Pepperjam/{client.ProgramId}/PSA/{endpoint}',\n",
        "            DateStart = None,\n",
        "            DateEnd = None,\n",
        "            DeltaColumn = '',\n",
        "            MergeKeys = 't.id = s.id',\n",
        "            RelativeDate = None\n",
        "        )\n",
        "\n",
        "\n",
        "        if len(datalist) > 0:\n",
        "           mylogger.info(f'...Deserializing advertiser/publisher/category data...')\n",
        "           df = self.deserialize.ad_publisher_category_ds(datalist)\n",
        "\n",
        "           if type(df) != type(None):\n",
        "               self._WriteDataFrameToLake(df, client)\n",
        "           else:\n",
        "               mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "        else:\n",
        "           mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')    \n",
        "\n",
        "    def CopyDataToLake_AdvertiserPublisherPromotionalMethod(self, client: Client,datalist):\n",
        "        endpoint = 'publisher_promotional-method'\n",
        "        #Set Load Params\n",
        "        client.LoadParams = LoadParams(\n",
        "            IsDelta = False,\n",
        "            Endpoint = {endpoint},\n",
        "            DLPath_STG = f'/Pepperjam/{client.ProgramId}/STG/{endpoint}',\n",
        "            DLPath_PSA = f'/Pepperjam/{client.ProgramId}/PSA/{endpoint}',\n",
        "            DateStart = None,\n",
        "            DateEnd = None,\n",
        "            DeltaColumn = '',\n",
        "            MergeKeys = 't.id = s.id',\n",
        "            RelativeDate = None\n",
        "        )\n",
        "\n",
        "        \n",
        "\n",
        "        if len(datalist) > 0:\n",
        "            df = self.deserialize.ad_publisher_promo_method_ds(datalist)\n",
        "            mylogger.info(f'...Deserializing publisher/promotional-method data...')\n",
        "\n",
        "            if type(df) != type(None):\n",
        "                self._WriteDataFrameToLake(df, client)\n",
        "            else:\n",
        "                mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "        else:\n",
        "            mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')    \n",
        "\n",
        "\n",
        "    #------ AdvertiserTerm -----#\n",
        "    def CopyDataToLake_AdvertiserTerm(self, client: Client):\n",
        "        endpoint = 'term'\n",
        "        #Set Load Params\n",
        "        client.LoadParams = LoadParams(\n",
        "            IsDelta = False,\n",
        "            Endpoint = {endpoint},\n",
        "            DLPath_STG = f'/Pepperjam/{client.ProgramId}/STG/{endpoint}',\n",
        "            DLPath_PSA = f'/Pepperjam/{client.ProgramId}/PSA/{endpoint}',\n",
        "            DateStart = None,\n",
        "            DateEnd = None,\n",
        "            DeltaColumn = '',\n",
        "            MergeKeys = 't.id = s.id',\n",
        "            RelativeDate = None\n",
        "        )\n",
        "\n",
        "        all_records = self.dataAccess.getData(client = client, end_point = endpoint)\n",
        "\n",
        "        if len(all_records) > 0:\n",
        "            df = self.deserialize.ds(all_records)\n",
        "            mylogger.info(f'...Deserializing term data...')\n",
        "\n",
        "            if type(df) != type(None):\n",
        "                self._WriteDataFrameToLake(df, client)\n",
        "            else:\n",
        "                mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "        else:\n",
        "            mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')   \n",
        "\n",
        "\n",
        "    #------ AdvertiserGroup -----#\n",
        "    def CopyDataToLake_AdvertiserGroup(self, client: Client):\n",
        "        endpoint = 'group'\n",
        "        #Set Load Params\n",
        "        client.LoadParams = LoadParams(\n",
        "            IsDelta = False,\n",
        "            Endpoint = {endpoint},\n",
        "            DLPath_STG = f'/Pepperjam/{client.ProgramId}/STG/{endpoint}',\n",
        "            DLPath_PSA = f'/Pepperjam/{client.ProgramId}/PSA/{endpoint}',\n",
        "            DateStart = None,\n",
        "            DateEnd = None,\n",
        "            DeltaColumn = '',\n",
        "            MergeKeys = 't.id = s.id',\n",
        "            RelativeDate = None\n",
        "        )\n",
        "\n",
        "        all_records = self.dataAccess.getData(client = client, end_point = endpoint)\n",
        "\n",
        "        if len(all_records) > 0:\n",
        "            df = self.deserialize.ds(all_records)\n",
        "            mylogger.info(f'...Deserializing group data...')\n",
        "\n",
        "            if type(df) != type(None):\n",
        "                self._WriteDataFrameToLake(df, client)\n",
        "            else:\n",
        "                mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "        else:\n",
        "            mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')   \n",
        "\n",
        "\n",
        "    #------ Advertiser Report (requires date logic/Query Params) -----#\n",
        "    def CopyDataToLake_TransactionSummary(self, client: Client):\n",
        "        endpoint = 'report_transaction-summary'\n",
        "        #Set Load Params\n",
        "        client.LoadParams = LoadParams(\n",
        "            IsDelta = True,\n",
        "            Endpoint = {endpoint},\n",
        "            DLPath_STG = f'/Pepperjam/{client.ProgramId}/STG/{endpoint}',\n",
        "            DLPath_PSA = f'/Pepperjam/{client.ProgramId}/PSA/{endpoint}',\n",
        "            DateStart = None,\n",
        "            DateEnd = None,                                     \n",
        "            DeltaColumn = 'date',\n",
        "            MergeKeys = 't.publisher_id = s.publisher_id AND t.date = s.date',\n",
        "            RelativeDate = 12,   #PLEASE note: The period of time formed by the start date and the end date must not exceed 1 year (12 months)\n",
        "            QueryParams = '&groupBy=publisher_date'\n",
        "        )\n",
        "        self._SetDateStartEnd(client)\n",
        "        DateBounds = self._GetRelativeStartEndDates(client) # set bounds of 1Y\n",
        "        all_records = []\n",
        "        for first, last in DateBounds:\n",
        "            monthly_records = self.dataAccess.getParamData(\n",
        "                client = client,\n",
        "                start_date = first,\n",
        "                end_date = last,\n",
        "                end_point = 'report/transaction-summary'\n",
        "            )\n",
        "            all_records.extend(monthly_records)\n",
        "            \n",
        "        if len(all_records) > 0:\n",
        "            df = self.deserialize.ds(all_records)\n",
        "            mylogger.info(f'...Deserializing report/transaction-summary data...')\n",
        "            \n",
        "            if type(df) != type(None):\n",
        "                self._WriteDataFrameToLake(df, client)\n",
        "            else:\n",
        "                mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "        else:\n",
        "            mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "    def CopyDataToLake_ReportAccountTransactions(self, client: Client):\n",
        "        endpoint = 'report_account-transactions'\n",
        "        #Set Load Params\n",
        "        client.LoadParams = LoadParams(\n",
        "            IsDelta = False,\n",
        "            Endpoint = {endpoint},\n",
        "            DLPath_STG = f'/Pepperjam/{client.ProgramId}/STG/{endpoint}',\n",
        "            DLPath_PSA = f'/Pepperjam/{client.ProgramId}/PSA/{endpoint}',\n",
        "            DateStart = None,\n",
        "            DateEnd = None,\n",
        "            DeltaColumn = '',\n",
        "            MergeKeys = 't.id = s.id',\n",
        "            RelativeDate = 6    #PLEASE note: The period of time formed by the start date and the end date must not exceed 1 year\n",
        "        )\n",
        "        self._SetDateStartEnd(client)\n",
        "        DateBounds = self._GetRelativeStartEndDates(client)\n",
        "        all_records = []\n",
        "        for first, last in DateBounds:\n",
        "            monthly_records = self.dataAccess.getParamData(\n",
        "                client = client,\n",
        "                start_date = first,\n",
        "                end_date = last,\n",
        "                end_point = 'report/account-transactions'\n",
        "            )\n",
        "            all_records.extend(monthly_records)\n",
        "            \n",
        "        if len(all_records) > 0:\n",
        "            df = self.deserialize.ds(all_records)\n",
        "            mylogger.info(f'...Deserializing report/account-transactions data...')\n",
        "            \n",
        "            if type(df) != type(None):\n",
        "                self._WriteDataFrameToLake(df, client)\n",
        "            else:\n",
        "                mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "        else:\n",
        "            mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "    def CopyDataToLake_ReportCreativeDetails(self, client: Client):\n",
        "        endpoint = 'report_creative-details'\n",
        "        #Set Load Params\n",
        "        client.LoadParams = LoadParams(\n",
        "            IsDelta = False,                                                \n",
        "            Endpoint = {endpoint},\n",
        "            DLPath_STG = f'/Pepperjam/{client.ProgramId}/STG/{endpoint}',\n",
        "            DLPath_PSA = f'/Pepperjam/{client.ProgramId}/PSA/{endpoint}',\n",
        "            DateStart = None,\n",
        "            DateEnd = None,                                                 \n",
        "            DeltaColumn = '',   \n",
        "            MergeKeys = '',\n",
        "            RelativeDate = 12 #PLEASE note: The period of time formed by the start date and the end date must not exceed 1 year\n",
        "\n",
        "        )\n",
        "        self._SetDateStartEnd(client)\n",
        "        DateBounds = self._GetRelativeStartEndDates(client)\n",
        "        all_records = []\n",
        "        for first, last in DateBounds:\n",
        "            monthly_records = self.dataAccess.getParamData(\n",
        "                client = client,\n",
        "                start_date = first,\n",
        "                end_date = last,\n",
        "                end_point = 'report/creative-details'\n",
        "            )\n",
        "            all_records.extend(monthly_records)\n",
        "        \n",
        "        if len(all_records) > 0:\n",
        "            df = self.deserialize.ds(all_records)\n",
        "            mylogger.info(f'...Deserializing report/creative-details data...')\n",
        "            \n",
        "            if type(df) != type(None):\n",
        "                self._WriteDataFrameToLake(df, client)\n",
        "            else:\n",
        "                mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "        else:\n",
        "            mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "    \n",
        "    def CopyDataToLake_ReportItemizedTransactionSummary(self, client: Client):\n",
        "        endpoint = 'report_itemized-transaction-summary'\n",
        "        #Set Load Params\n",
        "        client.LoadParams = LoadParams(\n",
        "            IsDelta = False,                                      \n",
        "            Endpoint = {endpoint},\n",
        "            DLPath_STG = f'/Pepperjam/{client.ProgramId}/STG/{endpoint}',\n",
        "            DLPath_PSA = f'/Pepperjam/{client.ProgramId}/PSA/{endpoint}',\n",
        "            DateStart = None,\n",
        "            DateEnd = None,                                         \n",
        "            DeltaColumn = '', \n",
        "            MergeKeys =  '',\n",
        "            RelativeDate = 12   #Please note: The period of time formed by the start date and the end date must not exceed 1 year\n",
        "\n",
        "        )\n",
        "        self._SetDateStartEnd(client)\n",
        "        DateBounds = self._GetRelativeStartEndDates(client)\n",
        "        all_records = []\n",
        "        for first, last in DateBounds:\n",
        "            monthly_records = self.dataAccess.getParamData(\n",
        "                client = client,\n",
        "                start_date = first,\n",
        "                end_date = last,\n",
        "                end_point = 'report/itemized-transaction-summary'\n",
        "            )\n",
        "            all_records.extend(monthly_records)\n",
        "            \n",
        "        if len(all_records) > 0:\n",
        "            df = self.deserialize.ds(all_records)\n",
        "            mylogger.info(f'...Deserializing report/itemized-transaction-summary data...')\n",
        "            \n",
        "            if type(df) != type(None):\n",
        "                self._WriteDataFrameToLake(df, client)\n",
        "            else:\n",
        "                mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "        else:\n",
        "            mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "    # ---- Delta Loaded Tables in the solution ---- #\n",
        "    def CopyDataToLake_ReportItemizedTransactions(self, client: Client):\n",
        "        endpoint = 'report_itemized-transactions'\n",
        "        #Set Load Params\n",
        "        client.LoadParams = LoadParams(\n",
        "            IsDelta = True,                                        \n",
        "            Endpoint = {endpoint},\n",
        "            DLPath_STG = f'/Pepperjam/{client.ProgramId}/STG/{endpoint}',\n",
        "            DLPath_PSA = f'/Pepperjam/{client.ProgramId}/PSA/{endpoint}',\n",
        "            DateStart = None,\n",
        "            DateEnd = None,                                                \n",
        "            DeltaColumn = 'sale_date',\n",
        "            MergeKeys = 't.transaction_id = s.transaction_id and t.item_id = s.item_id', \n",
        "            RelativeDate = 6 #PLEASE note: The period of time formed by the start date and the end date must not exceed 6 months\n",
        "        )\n",
        "        self._SetDateStartEnd(client)\n",
        "        DateBounds = self._GetRelativeStartEndDates(client) \n",
        "        all_records = []\n",
        "        for first, last in DateBounds:\n",
        "            monthly_records = self.dataAccess.getParamData(\n",
        "                client = client,\n",
        "                start_date = first,\n",
        "                end_date = last,\n",
        "                end_point = 'report/itemized-transactions'\n",
        "            )\n",
        "            all_records.extend(monthly_records)\n",
        "            \n",
        "        if len(all_records) > 0:\n",
        "            df = self.deserialize.ds(all_records)\n",
        "            mylogger.info(f'...Deserializing report/itemized-transactions data...')\n",
        "            \n",
        "            if type(df) != type(None):\n",
        "                self._WriteDataFrameToLake(df, client)\n",
        "            else:\n",
        "                mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "        else:\n",
        "            mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        " \n",
        "    def CopyDataToLake_ReportTransactionHistory(self, client: Client):\n",
        "        endpoint = 'report_transaction-history'\n",
        "        #Set Load Params\n",
        "        client.LoadParams = LoadParams(\n",
        "            IsDelta = True,\n",
        "            Endpoint = {endpoint},\n",
        "            DLPath_STG = f'/Pepperjam/{client.ProgramId}/STG/{endpoint}',\n",
        "            DLPath_PSA = f'/Pepperjam/{client.ProgramId}/PSA/{endpoint}',\n",
        "            DateStart = None,\n",
        "            DateEnd = None,                                                 \n",
        "            DeltaColumn = 'process_date',                     \n",
        "            MergeKeys = '',     #Data has no true grain. Need to append daily (I believe)\n",
        "            RelativeDate = 1    #PLEASE note: The period of time formed by the start date and the end date must not exceed 1 months\n",
        "        )\n",
        "        self._SetDateStartEnd(client)\n",
        "        DateBounds = self._GetRelativeStartEndDates(client) # set bounds of 1M\n",
        "        all_records = []\n",
        "        for first, last in DateBounds:\n",
        "            monthly_records = self.dataAccess.getParamData(\n",
        "                client = client,\n",
        "                start_date = first,\n",
        "                end_date = last,\n",
        "                end_point = 'report/transaction-history'\n",
        "            )\n",
        "            all_records.extend(monthly_records)\n",
        "            \n",
        "        if len(all_records) > 0:\n",
        "            df = self.deserialize.ds(all_records)\n",
        "            mylogger.info(f'...Deserializing report/transaction-history data...')\n",
        "            if type(df) != type(None):\n",
        "                self._WriteDataFrameToLake(df, client)\n",
        "            else:\n",
        "                mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "        else:\n",
        "            mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "    def CopyDataToLake_TransactionDetail(self, client: Client):\n",
        "        endpoint = 'report_transaction-details'\n",
        "        #Set Load Params\n",
        "        client.LoadParams = LoadParams(\n",
        "            IsDelta = True,\n",
        "            Endpoint = {endpoint},\n",
        "            DLPath_STG = f'/Pepperjam/{client.ProgramId}/STG/{endpoint}',\n",
        "            DLPath_PSA = f'/Pepperjam/{client.ProgramId}/PSA/{endpoint}',\n",
        "            DateStart = None,\n",
        "            DateEnd = None,\n",
        "            DeltaColumn = 'sale_date',\n",
        "            MergeKeys = 't.transaction_id = s.transaction_id',\n",
        "            RelativeDate = 6 #PLEASE note: The period of time formed by the start date and the end date must not exceed 6 months\n",
        "        )\n",
        "\n",
        "        self._SetDateStartEnd(client)\n",
        "\n",
        "        # Run the _Get6MStartEndDates function to get the appropriate months for the param\n",
        "        DateBounds = self._GetRelativeStartEndDates(client)\n",
        "        all_records = []\n",
        "\n",
        "        # Start for loop for each month in the bounds list\n",
        "        for (first, last) in DateBounds:\n",
        "            monthly_records = self.dataAccess.getParamData(\n",
        "                client = client,\n",
        "                start_date = first,\n",
        "                end_date = last,\n",
        "                end_point = 'report/transaction-details'\n",
        "            )\n",
        "            all_records.extend(monthly_records)\n",
        "\n",
        "        if len(all_records) > 0:\n",
        "            df = self.deserialize.ds(all_records)\n",
        "            mylogger.info(f'...Deserializing report/transaction-details data...')\n",
        "            \n",
        "            if type(df) != type(None):\n",
        "                self._WriteDataFrameToLake(df, client)\n",
        "            else:\n",
        "                mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "        else:\n",
        "            mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "\n",
        "    #------ Advertiser Creative -----#\n",
        "        #no pagination\n",
        "    def CopyDataToLake_AdvertiserCreativeGeneric(self, client: Client):\n",
        "        endpoint = 'creative_generic'\n",
        "        #Set Load Params\n",
        "        client.LoadParams = LoadParams(\n",
        "            IsDelta = False,\n",
        "            Endpoint = {endpoint},\n",
        "            DLPath_STG = f'/Pepperjam/{client.ProgramId}/STG/{endpoint}',\n",
        "            DLPath_PSA = f'/Pepperjam/{client.ProgramId}/PSA/{endpoint}',\n",
        "            DateStart = None,\n",
        "            DateEnd = None,\n",
        "            DeltaColumn = '',\n",
        "            MergeKeys = '',\n",
        "            RelativeDate = None\n",
        "        )\n",
        "\n",
        "        all_records = self.dataAccess.getNoPagination(client = client, end_point = endpoint)\n",
        "\n",
        "        if len(all_records) > 0:\n",
        "            df = self.deserialize.ds(all_records)\n",
        "            mylogger.info(f'...Deserializing creative/generic data...')\n",
        "\n",
        "            if type(df) != type(None):\n",
        "                self._WriteDataFrameToLake(df, client)\n",
        "            else:\n",
        "                mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "        else:\n",
        "            mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')  \n",
        "\n",
        "    def CopyDataToLake_AdvertiserCreativeText(self, client: Client):\n",
        "        endpoint = 'creative_text'\n",
        "        #Set Load Params\n",
        "        client.LoadParams = LoadParams(\n",
        "            IsDelta = False,\n",
        "            Endpoint = {endpoint},\n",
        "            DLPath_STG = f'/Pepperjam/{client.ProgramId}/STG/{endpoint}',\n",
        "            DLPath_PSA = f'/Pepperjam/{client.ProgramId}/PSA/{endpoint}',\n",
        "            DateStart = None,\n",
        "            DateEnd = None,\n",
        "            DeltaColumn = '',\n",
        "            MergeKeys = 't.id = s.id',\n",
        "            RelativeDate = None,\n",
        "            QueryParams = ''\n",
        "        )\n",
        "\n",
        "        all_records = self.dataAccess.getData(client = client, end_point = 'creative/text')\n",
        "\n",
        "        \n",
        "        if len(all_records) > 0:\n",
        "            df = self.deserialize.ds(all_records)\n",
        "            mylogger.info(f'...Deserializing creative/text data...')\n",
        "\n",
        "            if type(df) != type(None):\n",
        "                self._WriteDataFrameToLake(df, client) \n",
        "\n",
        "                self.CopyDataToLake_AdvertiserCreativeTextPrivateAffiliates(client,all_records)\n",
        "\n",
        "                self.CopyDataToLake_AdvertiserCreativeTextPromotions(client,all_records)\n",
        "                \n",
        "            else:\n",
        "                mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "        else:\n",
        "            mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')  \n",
        "\n",
        "    def CopyDataToLake_AdvertiserCreativeTextPrivateAffiliates(self, client: Client,datalist):\n",
        "        endpoint = 'creative_text_affiliates'\n",
        "        #Set Load Params\n",
        "        client.LoadParams = LoadParams(\n",
        "            IsDelta = False,\n",
        "            Endpoint = {endpoint},\n",
        "            DLPath_STG = f'/Pepperjam/{client.ProgramId}/STG/{endpoint}',\n",
        "            DLPath_PSA = f'/Pepperjam/{client.ProgramId}/PSA/{endpoint}',\n",
        "            DateStart = None,\n",
        "            DateEnd = None,\n",
        "            DeltaColumn = '',\n",
        "            MergeKeys = 't.id = s.id',\n",
        "            RelativeDate = None\n",
        "        )\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "        if len(datalist) > 0:\n",
        "            df = self.deserialize.creative_text_affiliates_ds(datalist)\n",
        "            mylogger.info(f'...Deserializing creative/text data...')\n",
        "\n",
        "            if type(df) != type(None):\n",
        "                self._WriteDataFrameToLake(df, client) \n",
        "            else:\n",
        "                mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "        else:\n",
        "            mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}') \n",
        "\n",
        "    def CopyDataToLake_AdvertiserCreativeTextPromotions(self, client: Client,datalist):\n",
        "        endpoint = 'creative_text_promotions'\n",
        "        #Set Load Params\n",
        "        client.LoadParams = LoadParams(\n",
        "            IsDelta = False,\n",
        "            Endpoint = {endpoint},\n",
        "            DLPath_STG = f'/Pepperjam/{client.ProgramId}/STG/{endpoint}',\n",
        "            DLPath_PSA = f'/Pepperjam/{client.ProgramId}/PSA/{endpoint}',\n",
        "            DateStart = None,\n",
        "            DateEnd = None,\n",
        "            DeltaColumn = '',\n",
        "            MergeKeys = 't.id = s.id',\n",
        "            RelativeDate = None\n",
        "        )\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "        if len(datalist) > 0:\n",
        "            df = self.deserialize.creative_text_promo_ds(datalist)\n",
        "            mylogger.info(f'...Deserializing creative/text data...')\n",
        "\n",
        "            if type(df) != type(None):\n",
        "                self._WriteDataFrameToLake(df, client) \n",
        "            else:\n",
        "                mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "        else:\n",
        "            mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}') \n",
        "\n",
        "    def CopyDataToLake_AdvertiserCreativeBanner(self, client: Client):\n",
        "        endpoint = 'creative_banner'\n",
        "        #Set Load Params\n",
        "        client.LoadParams = LoadParams(\n",
        "            IsDelta = False,\n",
        "            Endpoint = {endpoint},\n",
        "            DLPath_STG = f'/Pepperjam/{client.ProgramId}/STG/{endpoint}',\n",
        "            DLPath_PSA = f'/Pepperjam/{client.ProgramId}/PSA/{endpoint}',\n",
        "            DateStart = None,\n",
        "            DateEnd = None,\n",
        "            DeltaColumn = '',\n",
        "            MergeKeys = 't.id = s.id',\n",
        "            RelativeDate = None\n",
        "        )\n",
        "\n",
        "        all_records = self.dataAccess.getData(client = client, end_point='creative/banner')\n",
        "\n",
        "        if len(all_records) > 0:\n",
        "            mylogger.info(f'...Deserializing creative/banner data...')\n",
        "            df = self.deserialize.banner_ds(all_records)\n",
        "\n",
        "            if type(df) != type(None):\n",
        "                mylogger.info(f'...Writing Creative Banner to lake...') \n",
        "                self._WriteDataFrameToLake(df, client) \n",
        "                              \n",
        "            else:\n",
        "                mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "            self.CopyDataToLake_AdvertiserCreativeBannerPromotions(client,all_records)\n",
        "            self.CopyDataToLake_AdvertiserCreativeBannerPrivateAffiliates(client,all_records)\n",
        "  \n",
        "        else:\n",
        "            mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}') \n",
        "\n",
        "\n",
        "    def CopyDataToLake_AdvertiserCreativeBannerPromotions(self, client: Client, datalist):\n",
        "        endpoint = 'creative_banner-promotions'\n",
        "        #Set Load Params\n",
        "        client.LoadParams = LoadParams(\n",
        "            IsDelta = False,\n",
        "            Endpoint = {endpoint},\n",
        "            DLPath_STG = f'/Pepperjam/{client.ProgramId}/STG/{endpoint}',\n",
        "            DLPath_PSA = f'/Pepperjam/{client.ProgramId}/PSA/{endpoint}',\n",
        "            DateStart = None,\n",
        "            DateEnd = None,\n",
        "            DeltaColumn = '',\n",
        "            MergeKeys = 't.id = s.id',\n",
        "            RelativeDate = None\n",
        "        )\n",
        "        if len(datalist) > 0:\n",
        "            df = self.deserialize.banner_promotion_ds(datalist)\n",
        "            mylogger.info(f'...Deserializing creative/banner promotion data...')\n",
        "\n",
        "            if type(df) != type(None):\n",
        "                mylogger.info(f'...Writing Creative Banner Promotions to lake...')   \n",
        "                self._WriteDataFrameToLake(df, client)\n",
        "            else:\n",
        "                mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "        else:\n",
        "            mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}') \n",
        "\n",
        "    def CopyDataToLake_AdvertiserCreativeBannerPrivateAffiliates(self, client: Client, datalist):\n",
        "        endpoint = 'creative_banner-private-affiliates'\n",
        "        #Set Load Params\n",
        "        client.LoadParams = LoadParams(\n",
        "            IsDelta = False,\n",
        "            Endpoint = {endpoint},\n",
        "            DLPath_STG = f'/Pepperjam/{client.ProgramId}/STG/{endpoint}',\n",
        "            DLPath_PSA = f'/Pepperjam/{client.ProgramId}/PSA/{endpoint}',\n",
        "            DateStart = None,\n",
        "            DateEnd = None,\n",
        "            DeltaColumn = '',\n",
        "            MergeKeys = 't.id = s.id',\n",
        "            RelativeDate = None\n",
        "        )\n",
        "        if len(datalist) > 0:\n",
        "            df = self.deserialize.banner_private_aff_ds(datalist)\n",
        "            mylogger.info(f'...Deserializing creative/banner affiliate data...')\n",
        "\n",
        "            if type(df) != type(None):\n",
        "                mylogger.info(f'...Writing Creative Banner Affiliates to lake...')\n",
        "                self._WriteDataFrameToLake(df, client) \n",
        "            else:\n",
        "                mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "        else:\n",
        "            mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')  \n",
        "\n",
        "    def CopyDataToLake_AdvertiserCreativeCoupon(self, client: Client):\n",
        "        endpoint = 'creative_coupon'\n",
        "        #Set Load Params\n",
        "        client.LoadParams = LoadParams(\n",
        "            IsDelta = False,\n",
        "            Endpoint = {endpoint},\n",
        "            DLPath_STG = f'/Pepperjam/{client.ProgramId}/STG/{endpoint}',\n",
        "            DLPath_PSA = f'/Pepperjam/{client.ProgramId}/PSA/{endpoint}',\n",
        "            DateStart = None,\n",
        "            DateEnd = None,\n",
        "            DeltaColumn = '',\n",
        "            MergeKeys = 't.id = s.id',\n",
        "            RelativeDate = None\n",
        "        )\n",
        "\n",
        "        all_records = self.dataAccess.getData(client = client, end_point = 'creative/coupon')\n",
        "\n",
        "        if len(all_records) > 0:\n",
        "            df = self.deserialize.coupon_ds(all_records)\n",
        "            mylogger.info(f'...Deserializing creative/coupon data...')\n",
        "            if type(df) != type(None):\n",
        "                self._WriteDataFrameToLake(df, client)\n",
        "            else:\n",
        "                mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "            \n",
        "            \n",
        "            self.CopyDataToLake_AdvertiserCreativeCouponPrivateAffiliates(client,all_records)\n",
        "\n",
        "            self.CopyDataToLake_AdvertiserCreativeCouponPromotions(client,all_records)\n",
        "\n",
        "        else:\n",
        "            mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}') \n",
        "\n",
        "        \n",
        "\n",
        "    def CopyDataToLake_AdvertiserCreativeCouponPromotions(self, client: Client, datalist):\n",
        "        endpoint = 'creative_coupon-promotions'\n",
        "        #Set Load Params\n",
        "        client.LoadParams = LoadParams(\n",
        "            IsDelta = False,\n",
        "            Endpoint = {endpoint},\n",
        "            DLPath_STG = f'/Pepperjam/{client.ProgramId}/STG/{endpoint}',\n",
        "            DLPath_PSA = f'/Pepperjam/{client.ProgramId}/PSA/{endpoint}',\n",
        "            DateStart = None,\n",
        "            DateEnd = None,\n",
        "            DeltaColumn = '',\n",
        "            MergeKeys = 't.id = s.id',\n",
        "            RelativeDate = None\n",
        "        )\n",
        "        if len(datalist) > 0:\n",
        "            df = self.deserialize.coupon_promotion_ds(datalist)\n",
        "            mylogger.info(f'...Deserializing creative/coupon promotion data...')\n",
        "\n",
        "            if type(df) != type(None):\n",
        "                mylogger.info(f'...Writing Creative Coupon Promotions to lake...') \n",
        "                self._WriteDataFrameToLake(df, client)\n",
        "            else:\n",
        "                mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "        else:\n",
        "            mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}') \n",
        "\n",
        "    def CopyDataToLake_AdvertiserCreativeCouponPrivateAffiliates(self, client: Client, datalist):\n",
        "        endpoint = 'creative_coupon-private-affiliates'\n",
        "        #Set Load Params\n",
        "        client.LoadParams = LoadParams(\n",
        "            IsDelta = False,\n",
        "            Endpoint = {endpoint},\n",
        "            DLPath_STG = f'/Pepperjam/{client.ProgramId}/STG/{endpoint}',\n",
        "            DLPath_PSA = f'/Pepperjam/{client.ProgramId}/PSA/{endpoint}',\n",
        "            DateStart = None,\n",
        "            DateEnd = None,\n",
        "            DeltaColumn = '',\n",
        "            MergeKeys = 't.id = s.id',\n",
        "            RelativeDate = None\n",
        "        )\n",
        "        if len(datalist) > 0:\n",
        "            df = self.deserialize.coupon_private_aff_ds(datalist)\n",
        "            mylogger.info(f'...Deserializing creative/coupon affiliates data...')\n",
        "\n",
        "            if type(df) != type(None):\n",
        "                mylogger.info(f'...Writing Creative Coupon Affiliates to lake...')   \n",
        "                self._WriteDataFrameToLake(df, client)\n",
        "            else:\n",
        "                mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "        else:\n",
        "            mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}') \n",
        "\n",
        "\n",
        "    def CopyDataToLake_AdvertiserCreativeProduct(self, client: Client):\n",
        "        endpoint = 'creative_product'\n",
        "        #Set Load Params\n",
        "        client.LoadParams = LoadParams(\n",
        "            IsDelta = False,\n",
        "            Endpoint = {endpoint},\n",
        "            DLPath_STG = f'/Pepperjam/{client.ProgramId}/STG/{endpoint}',\n",
        "            DLPath_PSA = f'/Pepperjam/{client.ProgramId}/PSA/{endpoint}',\n",
        "            DateStart = None,\n",
        "            DateEnd = None,\n",
        "            DeltaColumn = '',\n",
        "            MergeKeys = '',\n",
        "            RelativeDate = None\n",
        "        )\n",
        "\n",
        "        all_records = self.dataAccess.getData(client = client, end_point='creative/product')\n",
        "\n",
        "        if len(all_records) > 0:\n",
        "            df = self.deserialize.ds(all_records)\n",
        "            mylogger.info(f'...Deserializing creative/product data...')\n",
        "\n",
        "            if type(df) != type(None):\n",
        "                self._WriteDataFrameToLake(df, client)\n",
        "            else:\n",
        "                mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "        else:\n",
        "            mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')  \n",
        "\n",
        "\n",
        "\n",
        "    def CopyDataToLake_AdvertiserCreativePromotion(self, client: Client):\n",
        "        endpoint = 'creative_promotion'\n",
        "        #Set Load Params\n",
        "        client.LoadParams = LoadParams(\n",
        "            IsDelta = False,\n",
        "            Endpoint = {endpoint},\n",
        "            DLPath_STG = f'/Pepperjam/{client.ProgramId}/STG/{endpoint}',\n",
        "            DLPath_PSA = f'/Pepperjam/{client.ProgramId}/PSA/{endpoint}',\n",
        "            DateStart = None,\n",
        "            DateEnd = None,\n",
        "            DeltaColumn = '',\n",
        "            MergeKeys = 't.id = s.id',\n",
        "            RelativeDate = None\n",
        "        )\n",
        "\n",
        "        all_records = self.dataAccess.getData(client = client, end_point = 'creative/promotion')\n",
        "\n",
        "        if len(all_records) > 0:\n",
        "            df = self.deserialize.ds(all_records)\n",
        "            mylogger.info(f'...Deserializing creative/promotion data...')\n",
        "\n",
        "            if type(df) != type(None):\n",
        "                self._WriteDataFrameToLake(df, client)\n",
        "            else:\n",
        "                mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')\n",
        "\n",
        "        else:\n",
        "            mylogger.info(f'No {client.LoadParams.Endpoint} data available for {client.ProgramName}-{client.ProgramId}')  \n",
        "\n",
        "\n",
        "    \n",
        "    #-------- Master Control Method - Load ALL-----#\n",
        "    def LoadAllClientData(self) -> None:\n",
        "        for client in self.clientSecretValues:\n",
        "            mylogger.info(f'Copying PJ data for client: {client.ProgramName}...')\n",
        "\n",
        "            #self.CopyDataToLake_AdvertiserPublisher(client)\n",
        "\n",
        "\n",
        "            #self.CopyDataToLake_AdvertiserTerm(client)\n",
        "\n",
        "            #self.CopyDataToLake_AdvertiserGroup(client)\n",
        "\n",
        "            #self.CopyDataToLake_ReportAccountTransactions(client)\n",
        "            #self.CopyDataToLake_ReportCreativeDetails(client)\n",
        "            #self.CopyDataToLake_ReportItemizedTransactionSummary(client)\n",
        "\n",
        "            #Delta Loaded Tables \n",
        "            #self.CopyDataToLake_ReportItemizedTransactions(client)\n",
        "            #self.CopyDataToLake_ReportTransactionHistory(client)\n",
        "            #self.CopyDataToLake_TransactionDetail(client)\n",
        "\n",
        "            #self.CopyDataToLake_AdvertiserCreativeGeneric(client)\n",
        "            #self.CopyDataToLake_AdvertiserCreativeText(client)\n",
        "            #self.CopyDataToLake_AdvertiserCreativeBanner(client)\n",
        "            #self.CopyDataToLake_AdvertiserCreativeCoupon(client)\n",
        "            #self.CopyDataToLake_AdvertiserCreativeProduct(client)\n",
        "            #self.CopyDataToLake_AdvertiserCreativePromotion(client)\n",
        "\n",
        "\n",
        "\n",
        "            mylogger.info(f'Finished copying PJ data for client: {client.ProgramName} \\n\\n')\n",
        "        mylogger.info(f'Finished copying PJ data for all available clients')\n",
        "\n",
        "    def LoadClientData(self,client: Client) -> None:\n",
        "        mylogger.info(f'Copying PJ data for client: {client.ProgramName}...')\n",
        "\n",
        "        self.CopyDataToLake_AdvertiserPublisher(client)\n",
        "\n",
        "\n",
        "        self.CopyDataToLake_AdvertiserTerm(client)\n",
        "\n",
        "        self.CopyDataToLake_AdvertiserGroup(client)\n",
        "\n",
        "        self.CopyDataToLake_ReportAccountTransactions(client)\n",
        "        self.CopyDataToLake_ReportCreativeDetails(client)\n",
        "        self.CopyDataToLake_ReportItemizedTransactionSummary(client)\n",
        "        self.CopyDataToLake_TransactionSummary(client)\n",
        "\n",
        "        #Delta Loaded Tables \n",
        "        self.CopyDataToLake_ReportItemizedTransactions(client)\n",
        "        self.CopyDataToLake_ReportTransactionHistory(client)\n",
        "        self.CopyDataToLake_TransactionDetail(client)\n",
        "\n",
        "        self.CopyDataToLake_AdvertiserCreativeGeneric(client)\n",
        "        self.CopyDataToLake_AdvertiserCreativeText(client)\n",
        "        self.CopyDataToLake_AdvertiserCreativeBanner(client)\n",
        "        self.CopyDataToLake_AdvertiserCreativeCoupon(client)\n",
        "        self.CopyDataToLake_AdvertiserCreativeProduct(client)\n",
        "        self.CopyDataToLake_AdvertiserCreativePromotion(client)\n",
        "\n",
        "\n",
        "\n",
        "        mylogger.info(f'Finished copying PJ data for client: {client.ProgramName} \\n\\n')\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def main():\r\n",
        "    PJAPIDataFetcher = PJ_ControlLogic()\r\n",
        "    PJAPIDataFetcher.LoadAllClientData()\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## **Run the main program to retrieve all data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "PJAPIDataFetcher = PJ_ControlLogic()\r\n",
        "def handle_client_data(client):\r\n",
        "    try:\r\n",
        "        PJAPIDataFetcher.LoadClientData(client)\r\n",
        "        return client, None  # Return None for error if successful\r\n",
        "    except Exception as e:\r\n",
        "        return client, e  # Return the exception if an error occurs\r\n",
        "\r\n",
        "# Declare or refine client list\r\n",
        "clientsToProcess = [client for client in PJAPIDataFetcher.clientSecretValues]\r\n",
        "# Debug a client ID\r\n",
        "# clientsToProcess = [client for client in PJAPIDataFetcher.clientSecretValues if client.ProgramId == '9310']\r\n",
        "\r\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\r\n",
        "    futures = {executor.submit(handle_client_data, client): client for client in clientsToProcess}\r\n",
        "\r\n",
        "    for future in concurrent.futures.as_completed(futures):\r\n",
        "        client = futures[future]  # Retrieve the client associated with this future\r\n",
        "        result, error = future.result()\r\n",
        "        if error:\r\n",
        "            mylogger.error(f'An error occurred on {client.ProgramName} - {client.ProgramId}: {str(error)}')\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#PJAPIDataFetcher = PJ_ControlLogic()\r\n",
        "#with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\r\n",
        "#    futures = [executor.submit(PJAPIDataFetcher.LoadClientData, client) for client in PJAPIDataFetcher.clientSecretValues]\r\n",
        "#\r\n",
        "#    for future in concurrent.futures.as_completed(futures):\r\n",
        "#        try:\r\n",
        "#            future.result()\r\n",
        "#        except Exception as e:\r\n",
        "#            mylogger.error(f'An error occured on {client.ProgramName} - {client.ProgramId}: {str(e)}')\r\n",
        "#\r\n",
        "#mylogger.info('All client data has been processed.\\n\\n')\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## **Testing cells**\r\n",
        "This area houses notebook cells which are in use to test the functionality of methods and functions above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Testing Cell\r\n",
        "\r\n",
        "# old copy activities \r\n",
        "#PJAPIDataFetcher = PJ_ControlLogic()\r\n",
        "## Does just what the LoadAllClientData() method does, but it explicitly calls each function for more visibility in back end logs\r\n",
        "##------ LOAD TABLES ------#BombshellSportswear - 9943\r\n",
        "#for client in PJAPIDataFetcher.clientSecretValues:\r\n",
        "#    if client.ProgramId == '9943':\r\n",
        "#        PJAPIDataFetcher.LoadClientData(client)\r\n",
        "#        mylogger.info(f'\\n\\nLOADING CLIENT {client.ProgramId}\\n\\n')\r\n",
        "#        PJAPIDataFetcher.CopyDataToLake_AdvertiserCreativeBanner(client)\r\n",
        "    #PJAPIDataFetcher.CopyDataToLake_AdvertiserCreativeCoupon(client)\r\n",
        "#    if client.ProgramId == '2337':\r\n",
        "#        \r\n",
        "#        PJAPIDataFetcher.CopyDataToLake_AdvertiserPublisher(client)\r\n",
        "#\r\n",
        "#\r\n",
        "#        PJAPIDataFetcher.CopyDataToLake_AdvertiserTerm(client)\r\n",
        "#\r\n",
        "#        PJAPIDataFetcher.CopyDataToLake_AdvertiserGroup(client)\r\n",
        "#\r\n",
        "#        PJAPIDataFetcher.CopyDataToLake_ReportAccountTransactions(client)\r\n",
        "#        PJAPIDataFetcher.CopyDataToLake_ReportCreativeDetails(client)\r\n",
        "#        PJAPIDataFetcher.CopyDataToLake_ReportItemizedTransactionSummary(client)\r\n",
        "#        PJAPIDataFetcher.CopyDataToLake_TransactionSummary(client)\r\n",
        "#\r\n",
        "#        #Delta Loaded Tables \r\n",
        "#        PJAPIDataFetcher.CopyDataToLake_ReportItemizedTransactions(client)\r\n",
        "#        PJAPIDataFetcher.CopyDataToLake_ReportTransactionHistory(client)\r\n",
        "#        PJAPIDataFetcher.CopyDataToLake_TransactionDetail(client)\r\n",
        "#\r\n",
        "#        PJAPIDataFetcher.CopyDataToLake_AdvertiserCreativeGeneric(client)\r\n",
        "#        PJAPIDataFetcher.CopyDataToLake_AdvertiserCreativeText(client)\r\n",
        "\r\n",
        "#        PJAPIDataFetcher.CopyDataToLake_AdvertiserCreativeProduct(client)\r\n",
        "#        PJAPIDataFetcher.CopyDataToLake_AdvertiserCreativePromotion(client)\r\n",
        "#\r\n",
        "##\r\n",
        "#mylogger.info(f'Data load finished')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\r\n",
        "\r\n",
        "#TestClient =  PJAPIDataFetcher.clientSecretValues[1]\r\n",
        "#data = PJAPIDataFetcher.CopyDataToLake_ReportCreativeDetails(TestClient) #grabbing the FOURTH value in the list \r\n",
        "#print(TestClient)\r\n",
        "# #PJAPIDataFetcher.LoadAllClientData(TestClient)\r\n",
        "\r\n",
        "\r\n",
        "# # publisher\r\n",
        "# PJAPIDataFetcher.CopyDataToLake_AdvertiserPublisher(TestClient)\r\n",
        "#data = PJAPIDataFetcher.CopyDataToLake_AdvertiserPublisherCategory(TestClient)\r\n",
        "# PJAPIDataFetcher.CopyDataToLake_AdvertiserPublisherPromotionalMethod(TestClient)\r\n",
        "\r\n",
        "# # term\r\n",
        "# PJAPIDataFetcher.CopyDataToLake_AdvertiserTerm(TestClient)\r\n",
        "\r\n",
        "# # Group\r\n",
        "# PJAPIDataFetcher.CopyDataToLake_AdvertiserGroup(TestClient)\r\n",
        "\r\n",
        "# #Report \r\n",
        "# PJAPIDataFetcher.CopyDataToLake_TransactionSummary(TestClient)\r\n",
        "# PJAPIDataFetcher.CopyDataToLake_ReportAccountTransactions(TestClient)\r\n",
        "# PJAPIDataFetcher.CopyDataToLake_ReportCreativeDetails(TestClient)\r\n",
        "# PJAPIDataFetcher.CopyDataToLake_ReportItemizedTransactionSummary(TestClient)\r\n",
        "\r\n",
        "\r\n",
        "# #Delta\r\n",
        "# PJAPIDataFetcher.CopyDataToLake_TransactionDetail(TestClient)\r\n",
        "# PJAPIDataFetcher.CopyDataToLake_ReportItemizedTransactions(TestClient)\r\n",
        "# PJAPIDataFetcher.CopyDataToLake_ReportTransactionHistory(TestClient)   #Removing for now since it takes the longest\r\n",
        "\r\n",
        "# # Creative\r\n",
        "# PJAPIDataFetcher.CopyDataToLake_AdvertiserCreativeGeneric(TestClient) #most clients have no response\r\n",
        "#ata =  PJAPIDataFetcher.CopyDataToLake_AdvertiserCreativeText(TestClient)\r\n",
        "#df = PJAPIDataFetcher.CopyDataToLake_AdvertiserCreativeBannerPromotions(TestClient)\r\n",
        "#df = PJAPIDataFetcher.CopyDataToLake_AdvertiserCreativeBanner(TestClient)\r\n",
        "#df = PJAPIDataFetcher.CopyDataToLake_AdvertiserCreativeCoupon(TestClient)\r\n",
        "# PJAPIDataFetcher.CopyDataToLake_AdvertiserCreativeProduct(TestClient)\r\n",
        "# PJAPIDataFetcher.CopyDataToLake_AdvertiserCreativePromotion(TestClient)\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\r\n",
        "#len(PJAPIDataFetcher.clientSecretValues)\r\n",
        "#df.show()\r\n",
        "#data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "    #advertiser/report/order-status             = Forbidden (Code: 403)\r\n",
        "    #advertiser/report/demand-detail            = Forbidden (Code: 403)\r\n",
        "    #advertiser/report/demand-summary           = Forbidden (Code: 403)\r\n",
        "    #advertiser/report/itemized-demand-detail   = Forbidden (Code: 403)\r\n",
        "    #advertiser/report/itemized-demand-summary  = Forbidden (Code: 403)\r\n",
        "\r\n",
        "\r\n",
        "    #advertiser/report/order-commission-rule    = dependent on transactionid\r\n",
        "    #advertiser/itemized-list                   = 2 records\r\n",
        "    #advertiser/itemized-list/product           = listId required\r\n",
        "    #advertiser/group/member                    = groupId required\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        ""
      ]
    }
  ]
}